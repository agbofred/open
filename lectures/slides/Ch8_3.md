---
title: "Emerging Recursion"
author: Jed Rembold
date: December 6, 2021
slideNumber: true
theme: "python_monokai"
highlightjs-theme: monokai
width: 1920
height: 1200
transition: slide
hash: true
history: false

---


## Announcements
- I'll let you know Wednesday's lecture format (remote or in-person) as soon as I can. I should have heard back on my test by tomorrow.
- I didn't make the progress I'd hoped on scoring this weekend, but I'm still trying to get caught up by the end of the week.
- I'll be pushing review materials to the website tomorrow
	- Will include a sample exam as well as objective list
- Polling: [rembold-class.ddns.net](http://rembold-class.ddns.net)


## Review Question {data-notes="Solution: 6 times"}
Last Friday we looked at the selection sort, which had following form:

```{.python data-line-numbers=""}
def selection_sort(array):
	for lh in range(len(array)):
		rh = lh
		for i in range(lh+1, len(array)):
			if array[i] < array[rh]:
				rh = i
		array[lh], array[rh] = array[rh], array[lh]
L = [3,2,6,1]
selection_sort(L)
```
How many times would line 5 be reached in sorting the shown list `L`?

:::{.hpoll}
#. 4
#. 6
#. 10
#. 16
:::

<!--
## Big-O Notation
- The common way to express notational complexity is to use _big-O notation_, introduced by German mathematician Paul Bachmann in 1892
- Big-O notations consists of the letter $\mathcal{O}$ followed by a formula that offers a qualitative assessment of the program running time as a function of the problem size ($N$)
- The complexity of linear search was:
$$\mathcal{O}(N)$$
- The complexity of selection sort was:
$$\mathcal{O}(N^2)$$
- Read aloud, these would be "big-O of $N$" or "big-O of $N^2$"


## Simplifying Big-O
- Big-O just gives a qualitative estimate, so it makes sense to keep the expression on the inside as simple as possible
- When writing big-O expressions, make the following simplifications:
	- Eliminate any constant factors
	- Eliminate any term whose contribution ceases to become significant when $N$ becomes large
- Thus the computational complexity of selection sort is
$$\mathcal{O}(N^2)$$
and not
$$\mathcal{O}\left(\frac{N^2 + N}{2}\right)$$


## Choosing Insignificant Terms
- For polynomials, higher order terms will generally be more significant, but be wary of potentially other expressions
- Can always make or look at a plot to help choose

<iframe src="https://www.desmos.com/calculator/tmdeiambhy?embed" width="80%" height="800px" style="border: 1px solid #ccc" frameborder=0></iframe>

-->


## A More Efficient Strategy
- As long as arrays are small, selection sort is perfectly workable!
	- Even 10,000 elements get sorted in just over 1 second
- Less attractive to commercial applications with **huge** arrays though
	- Sorting a million values to take over 3 hours?!
- The $\mathcal{O}(N^2)$ complexity does offer a little hope though
	- Sorting twice as many elements takes four times as long = BAD
	- But sorting half as many elements takes only a quarter the time = GOOD!
	- Can we break the array into smaller pieces and just work with those?


## A Diversion to Recursion
- _Recursion_ is the practice of dividing a problem up into smaller problems **of the same form**.
	- The sub-problems **must** have the same form as the larger problem, or it isn't recursion
- Since the sub-problems have the same form as the larger problem, they can be solved using the **same** function
	- From a code perspective, the defining characteristic of recursion is a function which calls itself internally

## Solving the same (simpler) problem
- Say you needed to fundraise a $1000 for an important cause
	- Tough to get a single $1000 donation
	- Could instead task 10 friends with raising $100
		- Each friend could task 10 other friends with raising $10
		- Then those friends could find 10 people who would each donate a dollar
- Solving the problem the same way each time:
	- Dividing up the problem and asking those people to solve a simpler (cheaper) problem
	- It **must** end somewhere, generally called the _base case_
		- Here that would have been at the $1 donation point

## Recursive Factorial
- The factorial function can be defined in two different ways, one of which lends itself nicely to recursion
$$n! = (n-1) \times (n-2) \times (n-3) \times \cdots \times 1$$
or
$$n! = \begin{cases} 1 & \text{if $n$ is 0} \\ n \times (n-1)! & \text{otherwise}\end{cases}$$
- The second can be expression computationally as:
```python
def factorial(n):
	if n == 0:	# Our base case
		return 1
	else:		# Our recursive case
		return n * factorial(n - 1)
```

## The Merge Sort Plan
#. Divide the array into two halves: `a1` and `a2`
#. Sort each of `a1` and `a2` recursively
#. Merge elements into the original array by choosing the smallest element from `a1` or `a2` on each cycle

We'll follow this process all the way through on pen and paper now to see that it works!


## Merge Sort Walkthrough
<div class="fig-container" data-file="https://visualgo.net/en/sorting" data-style="width:100%; height:900px;"></div>
<!--<div class="fig-container" data-file="https://www.hackerearth.com/practice/algorithms/sorting/merge-sort/visualize/&output=embed"></div>-->
<!--<iframe src="https://www.hackerearth.com/practice/algorithms/sorting/merge-sort/visualize/&output=embed"></iframe>-->


## Merge Sort Code Implementation
```{.python style='max-height:900px'}
def merge_sort(array):
    if len(array) > 1:
        mid = len(array)//2
        a1 = array[:mid]
        a2 = array[mid:]
        merge_sort(a1)
        merge_sort(a2)
        merge(array, a1, a2)

def merge(array, a1, a2):
    n1 = len(a1)
    n2 = len(a2)
    i1 = 0 #current front indices
    i2 = 0
    for i in range(len(array)):
        if (i1 < n1 and a1[i1] < a2[i2]) or i2 == n2:
            array[i] = a1[i1]
            i1 += 1
        else:
            array[i] = a2[i2]
            i2 += 1
```





## Merge Sort Complexity
![\ ](../images/MergeSortComplexity.svg)

- The work done at each level (all the work done by calls at that level) is proportional to the size of the array
- Running time is therefore proportional to $N$ times the number of levels


## So how many levels?
- Same division by two situation we had with Binary Search!
	- Equal to the number of times you can divide the array in half until only a single element remaining
	- Number of steps $k$ was given by:
	$$k = \log_2(N)$$
- Total complexity of merge sort is thus
$$\mathcal{O}(N \log_2(N))$$


## Comparing

$N$ | $N^2$ | $N\log_2 N$
---|---|---
10 | 100 | 33
100 | 10,000| 664
1,000 | 1,000,000| 9,966
10,000 | 100,000,000 | 132,877
100,000 | 10,000,000,000 | 1,660,964
1,000,000 | 1,000,000,000,000 | 19,931,569

- Based on this, the merge sort would be over 50,000x faster than selection sort for an array of 1 million values!

## Standard Complexity Classes
- Most tend to fall into one of a small number of complexity classes

Name | Big-O | Example
---|---|---
constant | $\mathcal{O}(1)$ | Finding the first element of an array
logarithmic | $\mathcal{O}(\log N)$ | Binary search in a sorted array
linear | $\mathcal{O}(N)$ | Summing over an array, or linear search
$N \log N$ | $\mathcal{O}(N\log N)$ | Merge sort
quadratic | $\mathcal{O}(N^2)$ | Selection sort
cubic | $\mathcal{O}(N^3)$ | Obvious algorithms for matrix multiplication
exponential | $\mathcal{O}(2^N)$ | Branch and try all possibilities

- In general, any problem who complexity can not be expressed as a polynomial is considered _intractable_


## P vs NP
![\ ](../images/P_NP.svg)


